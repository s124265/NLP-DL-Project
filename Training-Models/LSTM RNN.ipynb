{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of the LSTM RNN from Bowman et al. (https://nlp.stanford.edu/pubs/snli_paper.pdf) for text classification on the SST dataset and Textual entailment on the SNLI dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialzation\n",
    "\n",
    "Required packages and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax, relu, tanh\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pad_sequence as pad\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_sentences(batch):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [\" \".join([TEXT.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.text[:,i])])\n",
    "            for i in range(batch.text.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST\n",
    "### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, include_lengths = True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train_set, validation_set, test_set = datasets.SST.splits(TEXT,\n",
    "                                                          LABEL,\n",
    "                                                          fine_grained=False,\n",
    "                                                          train_subtrees=True,\n",
    "                                                          filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "LABEL.build_vocab(train_set)\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "    (train_set, validation_set, test_set), batch_size=32, sort_key=lambda x: len(x.text),sort_within_batch=True)\n",
    "#batchsst = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNet(\n",
      "  (embeddings): Embedding(18005, 300, padding_idx=1, max_norm=1)\n",
      "  (lstm_input): LSTM(300, 500)\n",
      "  (input): Linear(in_features=500, out_features=200, bias=True)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (batchnorm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.4)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.4\n",
    "input_dim = 200\n",
    "con_dim = 200\n",
    "hid_size = 500\n",
    "\n",
    "# build the LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim,padding_idx=1,max_norm=1)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "        # Simple RNN LSTM Layer\n",
    "        self.lstm_input = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = hid_size)\n",
    "        \n",
    "        # Linear layer (with tanh activation) for mapping to lower dimensions\n",
    "        self.input = Linear(in_features = hid_size,\n",
    "                             out_features = input_dim,\n",
    "                             bias = True)\n",
    "        \n",
    "        # Three stacked linear layers (with tanh activation)\n",
    "        self.l_1 = Linear(in_features=input_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = True)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=True)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = True)\n",
    "        self.batchnorm = nn.BatchNorm1d(num_features=con_dim)\n",
    "        \n",
    "        \n",
    "        # Applied dropout\n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # Putput layer\n",
    "        self.l_out = Linear(in_features= con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        \n",
    "        # input = (text_tensor, seq_length_tensor)\n",
    "        x_text = x[0].cuda() #Text of input\n",
    "       # y_text = y[0]\n",
    "        x_len = x[1].cuda() #Sequence length of input\n",
    "       # y_len = y[1]\n",
    "    \n",
    "        x_len, idx_sort = np.sort(x_len)[::-1],np.argsort(-x_len)\n",
    "        x_text = x_text.index_select(1,torch.LongTensor(idx_sort).cuda())\n",
    "    \n",
    "        #x_text = self.drop(x_text)\n",
    "        # Get embeddings\n",
    "        x = self.embeddings(x_text) # (batch size, length, embedding dim)\n",
    "       # y = self.embeddings(y_text)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # Applied dropout\n",
    "       # x = self.drop(x)\n",
    "       # y = self.drop(y)\n",
    "\n",
    "        # Packing padded sequences to max_length\n",
    "        packed_x = pack(x,x_len, batch_first = False)\n",
    "       # packed_y = pack(y,y_len.view(-1).tolist(), batch_first = False)\n",
    "       # packed_x = self.drop(packed_x)\n",
    "        \n",
    "        # LSTM RNN Layer\n",
    "        x = self.lstm_input(packed_x)[1][0].squeeze(0)\n",
    "       # y, ht_y0 = self.lstm_input(packed_y)\n",
    "        \n",
    "       # x = self.drop(x)\n",
    "        \n",
    "        # Unpacking packed tensors\n",
    "       #unpacked_x, unpacked_len = unpack(x, batch_first = False)\n",
    "       # unpacked_y, unpacked_len = unpack(y, batch_first = False)\n",
    "        xt = x\n",
    "       # xt = torch.squeeze(xt,0)\n",
    "       # print(xt.size())\n",
    "        \n",
    "       # xt = self.drop(xt)\n",
    "       # yt = unpacked_y\n",
    "        \n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        xt = xt.index_select(0,torch.LongTensor(idx_unsort).cuda())\n",
    "        \n",
    "        xt = self.drop(xt)\n",
    "        \n",
    "     #   xt = torch.cat((torch.mean(xt,dim=0),torch.max(xt,dim=0)[0]),dim=1)\n",
    "        #print(xt.size())\n",
    "        \n",
    "        \n",
    "        # Mapping input from 300 dim to 100 dim and concatenating\n",
    "        xt = tanh(self.input(xt))\n",
    "      #  yt = torch.tanh(self.input(yt))\n",
    "        \n",
    "        z = xt\n",
    "        \n",
    "        #\n",
    "        # Three stacked tanh layers\n",
    "        z = tanh(self.l_1(z))     \n",
    "        z = tanh(self.l_2(z))\n",
    "        z = tanh(self.l_3(z))\n",
    "        \n",
    "        z = self.batchnorm(z)\n",
    " \n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(z))\n",
    "        return out\n",
    "\n",
    "net = LSTMNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{p[0]: p[1].requires_grad for p in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederik\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\Frederik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid, it: 0 loss: 1.10 accs: 0.51\n",
      "\n",
      "train, it: 0 loss: 1.10 accs: 0.34\n",
      "train, it: 200 loss: 0.94 accs: 0.63\n",
      "train, it: 400 loss: 0.81 accs: 0.75\n",
      "valid, it: 500 loss: 0.79 accs: 0.76\n",
      "\n",
      "train, it: 600 loss: 0.76 accs: 0.79\n",
      "train, it: 800 loss: 0.75 accs: 0.80\n",
      "valid, it: 1000 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 1000 loss: 0.75 accs: 0.80\n",
      "train, it: 1200 loss: 0.74 accs: 0.81\n",
      "train, it: 1400 loss: 0.73 accs: 0.81\n",
      "valid, it: 1500 loss: 0.80 accs: 0.75\n",
      "\n",
      "train, it: 1600 loss: 0.72 accs: 0.83\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5f4c60c468de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4c4ba18bc4f4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# input = (text_tensor, seq_length_tensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Text of input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m        \u001b[1;31m# y_text = y[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Sequence length of input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.0003,amsgrad=True)\n",
    "\n",
    "max_iter = 10000\n",
    "eval_every = 500\n",
    "log_every = 200\n",
    "\n",
    "\n",
    "train_loss, train_accs, train_iter_list = [], [], []\n",
    "train_loss_list, train_accs_list = [],[]\n",
    "val_loss_list, val_accs_list, val_iter_list = [],[], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.text)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(val_batch.label.size())\n",
    "          #  print(val_batch.label.size())\n",
    "            val_losses += criterion(output['out'], val_batch.label.cuda()) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label.cuda()) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            \n",
    "       #     for key, _val in output.items():\n",
    "       #         if key not in val_meta:\n",
    "       #             val_meta[key] = []\n",
    "       #         val_meta[key].append(get_numpy(_val)) \n",
    "       #     val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "       #     val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "       #     val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "       # for key, _val in val_meta.items():\n",
    "       #     val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        val_loss_list.append(get_numpy(val_losses))\n",
    "        val_accs_list.append(get_numpy(val_accs))\n",
    "        val_iter_list.append(i)\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
<<<<<<< HEAD
    "    output = net(batch.text)\n",
=======
    "    output = net(batchsst.text)\n",
>>>>>>> 96edf7d875969a839eb5ba49e950540ec7e75ded
    "    batch_loss = criterion(output['out'], batch.label.cuda())\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label.cuda())))\n",
    " \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(net.parameters(),max_norm=5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        train_accs_list.append(np.mean(train_accs))    \n",
    "        train_iter_list.append(i)\n",
    "        train_loss, train_accs = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run test on SNLI...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a18dcea86c4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run test on SNLI...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTEXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParsedTextField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mTREE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mShiftReduceField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Run test on SNLI...\")\n",
    "TEXT = datasets.nli.ParsedTextField()\n",
    "LABEL = data.Field(sequential=False)\n",
    "TREE = datasets.nli.ShiftReduceField()\n",
    "\n",
    "train, validation, test = datasets.SNLI.splits(TEXT, LABEL,TREE)\n",
    "\n",
    "print(\"Fields:\", train.fields)\n",
    "print(\"Number of examples:\\n\", len(train))\n",
    "print(\"First Example instance:\\n\", vars(train[0]))\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "#TEXT.build_vocab(train,vectors=GloVe[name='840B',dim='300'])\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "    (train, validation, test), batch_size=3, repeat=False)\n",
    "# sort_key=lambda x:len(x.premise),sort_within_batch=True, \n",
    "#batch = next(iter(train_iter))\n",
    "#print(\"Numericalize premises:\\n\", batch.premise)\n",
    "#print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "#print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "print(\"Test iters function\")\n",
    "#train_iter, val_iter, test_iter = datasets.SNLI.iters(batch_size=4, trees=True)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Numericalize premises:\\n\", batch.premise)\n",
    "print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "#use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "input_dim = 100\n",
    "con_dim = 200\n",
    "\n",
    "\n",
    "# build the LSTM model\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "\n",
    "        # Simple RNN LSTM Layer\n",
    "        self.lstm_input = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = embedding_dim,\n",
    "                                  num_layers = 1,\n",
    "                                  batch_first = False)\n",
    "        \n",
    "        # Linear layer (with tanh activation) for mapping to lower dimensions\n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = False)\n",
    "        \n",
    "        # Three stacked linear layers (with tanh activation)\n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = False)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=False)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = False)\n",
    "        \n",
    "        \n",
    "        # Applied dropout\n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # Putput layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        \n",
    "       # print(x)\n",
    "        # input = (text_tensor, seq_length_tensor)\n",
    "        x_text = x[0].cuda() #Text of input\n",
    "       # print(x_text)\n",
    "        y_text = y[0].cuda()\n",
    "        x_len = x[1].cuda() #Sequence length of input\n",
    "        #print(x_len)\n",
    "        y_len = y[1].cuda()\n",
    "        #print(x_len)\n",
    "                \n",
    "        # Get embeddings\n",
    "        x = self.embeddings(x_text) # (batch size, length, embedding dim)\n",
    "        y = self.embeddings(y_text)\n",
    "\n",
    "        # Applied dropout\n",
    "        x = self.drop(x)\n",
    "        y = self.drop(y)\n",
    "        \n",
    "        \n",
    "\n",
    "        pad_zeros = pad([x,y], batch_first = False)\n",
    "        pad_zeros = pad_zeros.transpose(0, 1)\n",
    "        x = pad_zeros[0]\n",
    "        y = pad_zeros[1]\n",
    "        \n",
    "       # print(\"Pad zeros\")\n",
    "       # print(x.size())\n",
    "       # print(y.size())\n",
    "        \n",
    "        length = []\n",
    "        batch_size = 3\n",
    "        for i in range(batch_size):\n",
    "            length.append(x.size()[0])\n",
    "        # Packing padded sequences to max_length\n",
    "        packed_x = pack(x,length, batch_first = False)\n",
    "        packed_y = pack(y,length, batch_first = False)\n",
    "       # print(\"Packed\")\n",
    "       # print(packed_x[0].size())\n",
    "       # print(packed_y[0].size())\n",
    "        \n",
    "        # LSTM RNN Layer\n",
    "        x, ht_x0 = self.lstm_input(packed_x)\n",
    "        y, ht_y0 = self.lstm_input(packed_y)\n",
    "        #print(\"LSTM\")\n",
    "        #print(x[0].size())\n",
    "        #print(x[0].size())\n",
    "        \n",
    "        # Unpacking packed tensors\n",
    "        unpacked_x, unpacked_len = unpack(x, batch_first = False)\n",
    "        unpacked_y, unpacked_len = unpack(y, batch_first = False)\n",
    "        #print(\"Unpack\")\n",
    "        #print(unpacked_x.size())\n",
    "        #print(unpacked_y.size())\n",
    "        xt = unpacked_x\n",
    "        yt = unpacked_y\n",
    "\n",
    "        # Mapping input from 300 dim to 100 dim and concatenating\n",
    "        xt = torch.tanh(self.input(xt)) #(4,3,100)\n",
    "        yt = torch.tanh(self.input(yt)) #(6,3,100)\n",
    "        #print(\"Linear layer\")\n",
    "        #print(xt.size())\n",
    "        #print(yt.size())\n",
    "        \n",
    "       # pad_zeros = pad([xt,yt], batch_first = False)\n",
    "       # pad_zeros = pad_zeros.transpose(0, 1)\n",
    "        #xt = pad_zeros[0]\n",
    "        #yt = pad_zeros[1]\n",
    "       # print(\"Pad zeros for concat\")\n",
    "       # print(xt.size())\n",
    "       # print(yt.size)\n",
    "        z = torch.cat((xt,yt),2)\n",
    "        \n",
    "        # Three stacked tanh layers\n",
    "        z = torch.tanh(self.l_1(z))     \n",
    "        z = torch.tanh(self.l_2(z))\n",
    "        z = torch.tanh(self.l_3(z))\n",
    " \n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(z[0]), 1)\n",
    "        return out\n",
    "\n",
    "net = LSTMNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.01,weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "max_iter = 1\n",
    "eval_every = 500\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "       # val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.premise,val_batch.hypothesis)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(output['out'])\n",
    "            #print(val_batch.label)\n",
    "            val_losses += criterion(output['out'], val_batch.label.cuda()) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label.cuda()) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "           # print(val_batch.batch_size)\n",
    "            \n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batch.premise,batch.hypothesis)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
