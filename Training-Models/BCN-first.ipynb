{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax, relu, tanh\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train_set, validation_set, test_set = datasets.SST.splits(TEXT,\n",
    "                                                          LABEL,\n",
    "                                                          fine_grained=False,\n",
    "                                                          train_subtrees=True,\n",
    "                                                          filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x0000020E8A97AEB8>, 'label': <torchtext.data.field.Field object at 0x0000020E8A97AEF0>}\n",
      "len(train) 98794\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n",
      "\n",
      "Example 2 {'text': ['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train_set.fields)\n",
    "print('len(train)', len(train_set))\n",
    "print('vars(train[0])', vars(train_set[0]))\n",
    "print()\n",
    "print('Example 2', vars(train_set[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 18005\n",
      "TEXT.vocab.vectors.size() torch.Size([18005, 300])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "#url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "TEXT.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "#TEXT.build_vocab(train_set, vectors=Vectors('glove.840B.300d.txt',url = url))\n",
    "LABEL.build_vocab(train_set)\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3])\n",
      "tensor([1, 1, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_set, validation_set, test_set), batch_size=3)\n",
    "\n",
    "\n",
    "# print batch information\n",
    "batchsst = next(iter(train_iter))\n",
    "print(batchsst.text.size())\n",
    "print(batchsst.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCNNet(\n",
      "  (embeddings): Embedding(18005, 300)\n",
      "  (input): Linear(in_features=300, out_features=300, bias=False)\n",
      "  (bilstm_enc): LSTM(300, 300, bidirectional=True)\n",
      "  (enc_h2l): Linear(in_features=600, out_features=3, bias=False)\n",
      "  (bilstm_int): LSTM(300, 300, bidirectional=True)\n",
      "  (int_h2l): Linear(in_features=600, out_features=3, bias=True)\n",
      "  (maxpool): MaxPool1d(kernel_size=300, stride=300, padding=0, dilation=1, ceil_mode=False)\n",
      "  (avgpool): AvgPool1d(kernel_size=(300,), stride=(300,), padding=(0,))\n",
      "  (maxout): Linear(in_features=18, out_features=18, bias=False)\n",
      "  (out): Linear(in_features=18, out_features=3, bias=False)\n",
      "  (drop): Dropout(p=0.2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "hidden_dim = 300\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "# build the BoW model\n",
    "class BCNNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BCNNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        #self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # The ReLu activtion layer\n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                            out_features = embedding_dim,\n",
    "                             bias = False)\n",
    "        \n",
    "        # bilstm encoder\n",
    "        self.bilstm_enc = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = embedding_dim,\n",
    "                                  bidirectional = True)\n",
    "       # self.hidenc_x = (Variable(torch.zeros(2, batch_size, hidden_dim)),\n",
    "       #                  Variable(torch.zeros(2, batch_size, hidden_dim)))\n",
    "       # self.hidenc_y = (Variable(torch.zeros(2, batch_size, hidden_dim)),\n",
    "       #                  Variable(torch.zeros(2, batch_size, hidden_dim)))\n",
    "        self.enc_h2l = Linear(in_features = hidden_dim*2,\n",
    "                          out_features = num_classes,\n",
    "                             bias = False)\n",
    "        \n",
    "        # bilstm integrator\n",
    "        self.bilstm_int = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = embedding_dim,\n",
    "                                  bidirectional = True)\n",
    "       # self.hidint_x = (Variable(torch.zeros(2, batch_size, hidden_dim)),\n",
    "       #                  Variable(torch.zeros(2, batch_size, hidden_dim)))\n",
    "       # self.hidint_y = (Variable(torch.zeros(2, batch_size, hidden_dim)),\n",
    "       #                  Variable(torch.zeros(2, batch_size, hidden_dim)))\n",
    "        self.int_h2l = Linear(in_features = hidden_dim*2,\n",
    "                              out_features = num_classes)\n",
    "        \n",
    "        #Pooling\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = embedding_dim)\n",
    "        self.avgpool = nn.AvgPool1d(kernel_size = embedding_dim)\n",
    "        \n",
    "        #Batchnorm\n",
    "        #self.batchnorm = nn.BatchNorm1d()\n",
    "                \n",
    "        # maxout layer\n",
    "        self.maxout = Linear(in_features = num_classes*6,\n",
    "                                out_features = num_classes*6,\n",
    "                                bias = False)\n",
    "        self.out = Linear(in_features = num_classes*6,\n",
    "                                out_features = num_classes,\n",
    "                                \n",
    "                                bias = False)\n",
    "        \n",
    "        # dropout\n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embeddings(x)\n",
    "        y = self.embeddings(y)\n",
    "        \n",
    "        # Relu\n",
    "        x = relu(self.input(x))\n",
    "        y = relu(self.input(y))\n",
    "        \n",
    "        # Encoder\n",
    "        # biLSTM\n",
    "        x, hidenc_fx = self.bilstm_enc(x)\n",
    "        y, hidenc_fy = self.bilstm_enc(y)\n",
    "        \n",
    "        X = (hidenc_fx[0])\n",
    "        Y = (hidenc_fy[0])\n",
    "        \n",
    "        # Biattention\n",
    "        # X & Y ?\n",
    "        #dim 1 = len, dim 2 = dim, and they are swaped so we can multiply the matrices\n",
    "        A = torch.matmul(X, Y.transpose(1,2)) # A = X * Y^T  x = bs, len, dim, y^T = bs,dim, len\n",
    "        A_x = softmax(A) # A_x = softmax(A)\n",
    "        A_y = softmax(A.transpose(1,2)) # A_y = softmax(A^T)\n",
    "        C_x = torch.matmul(A_x.transpose(1,2), X) # C_x = A_x^T * X \n",
    "        C_y = torch.matmul(A_y.transpose(1,2), Y) # C_y = A_y^T * Y\n",
    "        \n",
    "        # Integrator\n",
    "        # input for integrator bilstm\n",
    "        x = torch.cat((X, X-C_y, torch.mul(X, C_y)), 2) # Concat([X; X-C_y; X.C_y])\n",
    "        y = torch.cat((Y, Y-C_x, torch.mul(Y, C_x)), 2) # Concat([Y; Y-C_x; Y.C_x]\n",
    "        # X_|y = biLSTM(%), Y|x = biLSTM(%)\n",
    "        X_y, hidint_x  = self.bilstm_int(x)\n",
    "        Y_x, hidint_y = self.bilstm_int(y)\n",
    "        \n",
    "        # Pooling\n",
    "        # x_self = ?\n",
    "        # x_pool = Concat([max(X_|y); mean(X_|y); min(X_|y); x_self])\n",
    "        x_maxpool = self.maxpool(hidint_x[0])\n",
    "        x_meanpool = self.avgpool(hidint_x[0])\n",
    "        x_minpool = hidint_x[0] * -1\n",
    "        x_minpool = self.maxpool(x_minpool)\n",
    "        x_minpool = x_minpool * -1\n",
    "        x_pool = torch.cat((x_maxpool, x_meanpool, x_minpool), 2)\n",
    "        \n",
    "        y_maxpool = self.maxpool(hidint_y[0])\n",
    "        y_meanpool = self.avgpool(hidint_y[0])\n",
    "        y_minpool = hidint_y[0] * -1\n",
    "        y_minpool = self.maxpool(y_minpool)\n",
    "        y_minpool = y_minpool * -1\n",
    "        y_pool = torch.cat((y_maxpool, y_meanpool, y_minpool), 2)\n",
    "        \n",
    "        \n",
    "        print(x_pool.size())\n",
    "        print(y_pool.size())\n",
    "        # y_self = ?\n",
    "        # y_pool = Concat([max(Y_|x); mean(Y_|x); min(Y_|x); y_self])\n",
    "      #  y_pool = torch.cat((torch.max(hidint_y[0], 0), torch.mean(hidint_y[0], 0), torch.min(hidint_y[0], 0)), 0)\n",
    "        \n",
    "        # Maxout layer\n",
    "        # With dropout and batchnormilization\n",
    "        # Joined representations is the concatination of x_pool and y_pool\n",
    "        joined = torch.cat((x_pool, y_pool), 2)\n",
    "        print(joined.size())\n",
    "        joined = self.drop(joined)\n",
    "        #joined = torch.nn.BatchNorm1d(joined)\n",
    "        \n",
    "        # 1st Layer\n",
    "        print(joined.size())\n",
    "        #print(joined)\n",
    "        joined = torch.tanh(self.maxout(joined))\n",
    "        joined = self.drop(joined)\n",
    "        #joined = torch.nn.BatchNorm1d(joined)\n",
    "        print(joined.size())\n",
    "        \n",
    "        # 2nd layer\n",
    "        joined = torch.tanh(self.maxout(joined))\n",
    "        joined = self.drop(joined)\n",
    "        #joined = torch.nn.BatchNorm1d(joined)\n",
    "        print(joined.size())\n",
    "        \n",
    "        # 3rd maxout layer will be output\n",
    "        out['out'] = self.out(joined)\n",
    "        return out\n",
    "\n",
    "net = BCNNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.001,weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_sentences(batch):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [\" \".join([TEXT.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.text[:,i])])\n",
    "            for i in range(batch.text.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#derp = next(iter(val_iter))\n",
    "#print(derp.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING BCN WITH SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederik\\AppData\\Roaming\\Python\\Python36\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "C:\\Users\\Frederik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:94: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\Frederik\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 300, got 900",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-243-b04a0e61c27d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m       \u001b[1;31m#  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mval_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m# batches sizes might vary, which is why we cannot just mean the batch's loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# we multiply the loss and accuracies with the batch's size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-239-b7d38ece7fe5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mC_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Concat([Y; Y-C_x; Y.C_x]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;31m# X_|y = biLSTM(%), Y|x = biLSTM(%)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mX_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidint_x\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbilstm_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mY_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidint_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbilstm_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    128\u001b[0m             raise RuntimeError(\n\u001b[0;32m    129\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 130\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 300, got 900"
     ]
    }
   ],
   "source": [
    "max_iter = 1\n",
    "eval_every = 1000\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.text,val_batch.text)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(val_batch.label.size())\n",
    "          #  print(val_batch.label.size())\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            \n",
    "       #     for key, _val in output.items():\n",
    "       #         if key not in val_meta:\n",
    "       #             val_meta[key] = []\n",
    "       #         val_meta[key].append(get_numpy(_val)) \n",
    "       #     val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "       #     val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "       #     val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "       # for key, _val in val_meta.items():\n",
    "       #     val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batchsst.text,batchsst.text)\n",
    "    batch_loss = criterion(output['out'], batchsst.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batchsst.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Run test on SNLI...\")\n",
    "TEXT = datasets.snli.ParsedTextField()\n",
    "LABEL = data.LabelField()\n",
    "TREE = datasets.snli.ShiftReduceField()\n",
    "\n",
    "train, val, test = datasets.SNLI.splits(TEXT, LABEL, TREE)\n",
    "\n",
    "print(\"Fields:\", train.fields)\n",
    "print(\"Number of examples:\\n\", len(train))\n",
    "print(\"First Example instance:\\n\", vars(train[0]))\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train, max_size=None, vectors=[CharNGram(),Vectors('wiki.simple.vec', url=url)])\n",
    "#TEXT.build_vocab(train,vectors=GloVe[name='840B',dim='300'])\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_size=3)\n",
    "\n",
    "#batch = next(iter(train_iter))\n",
    "#print(\"Numericalize premises:\\n\", batch.premise)\n",
    "#print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "#print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "print(\"Test iters function\")\n",
    "train_iter, val_iter, test_iter = datasets.SNLI.iters(batch_size=4, trees=True)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Numericalize premises:\\n\", batch.premise)\n",
    "print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "#val_iter_set = next(iter(val_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train)\n",
    "#LABEL.build_vocab(train)\n",
    "print(TEXT.vocab.vectors.size()[0])\n",
    "print(len(LABEL.vocab.itos))\n",
    "#print(vars(TEXT.vocab))\n",
    "#print(vars(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "# build the BoW model\n",
    "class BoWNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BoWNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        #self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # add hidden layers\n",
    "        # YOUR CODE HERE!\n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = False)\n",
    "        \n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = False)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=False)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = False)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x) # (bs,len,300)\n",
    "        y = self.embeddings(y)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        y = self.drop(y)\n",
    "        \n",
    "        #sum_x =  # (bs,300) \n",
    "        \n",
    "        sum_x = torch.sum(x,0)\n",
    "        sum_y = torch.sum(y,0)\n",
    "        \n",
    "              \n",
    "        #tanh # (bs,100)\n",
    "        \n",
    "        sum_x = torch.tanh(self.input(sum_x))\n",
    "        sum_y = torch.tanh(self.input(sum_y))\n",
    "\n",
    "    \n",
    "        z = torch.cat((sum_x,sum_y),1)\n",
    "        \n",
    "        z = torch.tanh(self.l_1(z))     \n",
    "        z = torch.tanh(self.l_2(z))\n",
    "        z = torch.tanh(self.l_3(z))\n",
    "       # print(z.size())\n",
    "        \n",
    "       # z = self.drop(z)\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = self.l_out(z)\n",
    "        return out\n",
    "\n",
    "net = BoWNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.001,weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter = 5000\n",
    "eval_every = 500\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "       # val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.premise,val_batch.hypothesis)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(output['out'])\n",
    "            #print(val_batch.label)\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "           # print(val_batch.batch_size)\n",
    "            \n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batch.premise,batch.hypothesis)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
