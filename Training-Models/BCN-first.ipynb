{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'c9867dd6-23b7-4dbd-af46-a77e367fe4fb' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'c9867dd6-23b7-4dbd-af46-a77e367fe4fb' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.10.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.10.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.10.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"c9867dd6-23b7-4dbd-af46-a77e367fe4fb\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax, relu, tanh\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train_set, validation_set, test_set = datasets.SST.splits(TEXT,\n",
    "                                                          LABEL,\n",
    "                                                          fine_grained=False,\n",
    "                                                          train_subtrees=True,\n",
    "                                                          filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x000002B2296D62B0>, 'label': <torchtext.data.field.Field object at 0x000002B2296D6240>}\n",
      "len(train) 98794\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n",
      "\n",
      "Example 2 {'text': ['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train_set.fields)\n",
    "print('len(train)', len(train_set))\n",
    "print('vars(train[0])', vars(train_set[0]))\n",
    "print()\n",
    "print('Example 2', vars(train_set[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 18005\n",
      "TEXT.vocab.vectors.size() torch.Size([18005, 400])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "#url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "TEXT.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "#TEXT.build_vocab(train_set, vectors=Vectors('glove.840B.300d.txt',url = url))\n",
    "LABEL.build_vocab(train_set)\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([1, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_set, validation_set, test_set), batch_size=3)\n",
    "\n",
    "\n",
    "# print batch information\n",
    "batchsst = next(iter(train_iter))\n",
    "print(batchsst.text.size())\n",
    "print(batchsst.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWNet(\n",
      "  (embeddings): Embedding(18005, 400)\n",
      "  (input): Linear(in_features=400, out_features=100, bias=False)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "# build the BoW model\n",
    "class BCNNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BoWNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        #self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # The ReLu activtion layer\n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = False)\n",
    "        \n",
    "        # The encoder\n",
    "        self.enc = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = embedding_dim,\n",
    "                                  batch_first = False,\n",
    "                                  num_layers = 1\n",
    "                                  bidirectional = True)\n",
    "        \n",
    "        # The decoder / integrator\n",
    "        self.dec = nn.LSTM(input_size = embedding_dim,\n",
    "                                  hidden_size = embedding_dim,\n",
    "                                  batch_first = False,\n",
    "                                  num_layers = 1\n",
    "                                  bidirectional = True)\n",
    "        \n",
    "        #self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = self.l_out(z)\n",
    "        return out\n",
    "\n",
    "net = BCNNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.001,weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_sentences(batch):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [\" \".join([TEXT.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.text[:,i])])\n",
    "            for i in range(batch.text.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#derp = next(iter(val_iter))\n",
    "#print(derp.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING BOW WITH SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederik\\AppData\\Roaming\\Python\\Python36\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid, it: 0 loss: 1.11 accs: 0.25\n",
      "\n",
      "train, it: 0 loss: 1.13 accs: 0.00\n",
      "train, it: 200 loss: 1.02 accs: 0.69\n",
      "train, it: 400 loss: 0.73 accs: 1.00\n",
      "train, it: 600 loss: 0.41 accs: 1.00\n",
      "train, it: 800 loss: 0.20 accs: 1.00\n",
      "valid, it: 1000 loss: 1.13 accs: 0.49\n",
      "\n",
      "train, it: 1000 loss: 0.11 accs: 1.00\n"
     ]
    }
   ],
   "source": [
    "max_iter = 1\n",
    "eval_every = 1000\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.text,val_batch.text)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(val_batch.label.size())\n",
    "          #  print(val_batch.label.size())\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            \n",
    "       #     for key, _val in output.items():\n",
    "       #         if key not in val_meta:\n",
    "       #             val_meta[key] = []\n",
    "       #         val_meta[key].append(get_numpy(_val)) \n",
    "       #     val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "       #     val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "       #     val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "       # for key, _val in val_meta.items():\n",
    "       #     val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batchsst.text,batchsst.text)\n",
    "    batch_loss = criterion(output['out'], batchsst.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batchsst.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run test on SNLI...\n",
      "Fields: {'premise': <torchtext.datasets.snli.ParsedTextField object at 0x000002B1647FCD68>, 'premise_transitions': <torchtext.datasets.snli.ShiftReduceField object at 0x000002B164A215C0>, 'hypothesis': <torchtext.datasets.snli.ParsedTextField object at 0x000002B1647FCD68>, 'hypothesis_transitions': <torchtext.datasets.snli.ShiftReduceField object at 0x000002B164A215C0>, 'label': <torchtext.data.field.LabelField object at 0x000002B1647FCC18>}\n",
      "Number of examples:\n",
      " 549367\n",
      "First Example instance:\n",
      " {'premise': ['A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'premise_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'hypothesis': ['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'hypothesis_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'label': 'neutral'}\n",
      "Test iters function\n",
      "Numericalize premises:\n",
      " tensor([[    1,     1,     1,     1],\n",
      "        [    1,     2,     1,     1],\n",
      "        [    1,   969,     1,     2],\n",
      "        [    1,     3,     1,  2573],\n",
      "        [    1,    18,     1,  6592],\n",
      "        [    2,  6502,     1,     3],\n",
      "        [ 4838,  1043,     2,  2114],\n",
      "        [   12,  2455,   916,    65],\n",
      "        [  383,     3,  2249,     7],\n",
      "        [    3,    17,    16,   111],\n",
      "        [   50,  5560, 11410,     6],\n",
      "        [  331,   706,     3,    57],\n",
      "        [   81,   666,     5,   872],\n",
      "        [    7,   104,   503,   115],\n",
      "        [   18,     5,   639,     7],\n",
      "        [   33,    30,    31,     5],\n",
      "        [    6,    51,    21,   116],\n",
      "        [   15,    11,     8,    64],\n",
      "        [    4,    54,     4,    20]], device='cuda:0')\n",
      "Numericalize hypotheses:\n",
      " tensor([[    1,     1,     1,     1],\n",
      "        [    1,     1,     2,     1],\n",
      "        [    1,     1,  5192,     1],\n",
      "        [    2,     2, 10085,     1],\n",
      "        [  114,   969,   496,   115],\n",
      "        [    9,     3,    17,     7],\n",
      "        [ 1448,    18,   199,     5],\n",
      "        [   69,    11,     6,    43],\n",
      "        [   81,    30,     8,    11],\n",
      "        [   14,    20,    14,   308]], device='cuda:0')\n",
      "Entailment labels:\n",
      " tensor([2, 1, 3, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Run test on SNLI...\")\n",
    "TEXT = datasets.snli.ParsedTextField()\n",
    "LABEL = data.LabelField()\n",
    "TREE = datasets.snli.ShiftReduceField()\n",
    "\n",
    "train, val, test = datasets.SNLI.splits(TEXT, LABEL, TREE)\n",
    "\n",
    "print(\"Fields:\", train.fields)\n",
    "print(\"Number of examples:\\n\", len(train))\n",
    "print(\"First Example instance:\\n\", vars(train[0]))\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train, max_size=None, vectors=[CharNGram(),Vectors('wiki.simple.vec', url=url)])\n",
    "#TEXT.build_vocab(train,vectors=GloVe[name='840B',dim='300'])\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_size=3)\n",
    "\n",
    "#batch = next(iter(train_iter))\n",
    "#print(\"Numericalize premises:\\n\", batch.premise)\n",
    "#print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "#print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "print(\"Test iters function\")\n",
    "train_iter, val_iter, test_iter = datasets.SNLI.iters(batch_size=4, trees=True)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Numericalize premises:\\n\", batch.premise)\n",
    "print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "#val_iter_set = next(iter(val_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42392\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#TEXT.build_vocab(train)\n",
    "#LABEL.build_vocab(train)\n",
    "print(TEXT.vocab.vectors.size()[0])\n",
    "print(len(LABEL.vocab.itos))\n",
    "#print(vars(TEXT.vocab))\n",
    "#print(vars(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWNet(\n",
      "  (embeddings): Embedding(42392, 400)\n",
      "  (input): Linear(in_features=400, out_features=100, bias=False)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=False)\n",
      "  (drop): Dropout(p=0.5)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "# build the BoW model\n",
    "class BoWNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BoWNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        #self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # add hidden layers\n",
    "        # YOUR CODE HERE!\n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = False)\n",
    "        \n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = False)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=False)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = False)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x) # (bs,len,300)\n",
    "        y = self.embeddings(y)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        y = self.drop(y)\n",
    "        \n",
    "        #sum_x =  # (bs,300) \n",
    "        \n",
    "        sum_x = torch.sum(x,0)\n",
    "        sum_y = torch.sum(y,0)\n",
    "        \n",
    "              \n",
    "        #tanh # (bs,100)\n",
    "        \n",
    "        sum_x = torch.tanh(self.input(sum_x))\n",
    "        sum_y = torch.tanh(self.input(sum_y))\n",
    "\n",
    "    \n",
    "        z = torch.cat((sum_x,sum_y),1)\n",
    "        \n",
    "        z = torch.tanh(self.l_1(z))     \n",
    "        z = torch.tanh(self.l_2(z))\n",
    "        z = torch.tanh(self.l_3(z))\n",
    "       # print(z.size())\n",
    "        \n",
    "       # z = self.drop(z)\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = self.l_out(z)\n",
    "        return out\n",
    "\n",
    "net = BoWNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adadelta(net.parameters(), lr=0.001,weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frederik\\AppData\\Roaming\\Python\\Python36\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid, it: 0 loss: 1.01 accs: 0.26\n",
      "\n",
      "train, it: 0 loss: 1.12 accs: 0.25\n",
      "train, it: 200 loss: 1.01 accs: 0.29\n",
      "train, it: 400 loss: 1.01 accs: 0.29\n",
      "valid, it: 500 loss: 1.01 accs: 0.31\n",
      "\n",
      "train, it: 600 loss: 1.00 accs: 0.34\n",
      "train, it: 800 loss: 1.01 accs: 0.32\n",
      "valid, it: 1000 loss: 1.00 accs: 0.33\n",
      "\n",
      "train, it: 1000 loss: 1.01 accs: 0.31\n",
      "train, it: 1200 loss: 1.00 accs: 0.30\n",
      "train, it: 1400 loss: 0.99 accs: 0.33\n",
      "valid, it: 1500 loss: 1.00 accs: 0.34\n",
      "\n",
      "train, it: 1600 loss: 0.99 accs: 0.34\n",
      "train, it: 1800 loss: 0.99 accs: 0.35\n",
      "valid, it: 2000 loss: 1.01 accs: 0.34\n",
      "\n",
      "train, it: 2000 loss: 1.00 accs: 0.35\n",
      "train, it: 2200 loss: 0.98 accs: 0.35\n",
      "train, it: 2400 loss: 1.01 accs: 0.31\n",
      "valid, it: 2500 loss: 1.00 accs: 0.35\n",
      "\n",
      "train, it: 2600 loss: 1.01 accs: 0.36\n",
      "train, it: 2800 loss: 0.99 accs: 0.34\n",
      "valid, it: 3000 loss: 1.00 accs: 0.35\n",
      "\n",
      "train, it: 3000 loss: 1.01 accs: 0.34\n",
      "train, it: 3200 loss: 0.99 accs: 0.36\n",
      "train, it: 3400 loss: 0.99 accs: 0.34\n",
      "valid, it: 3500 loss: 1.01 accs: 0.35\n",
      "\n",
      "train, it: 3600 loss: 1.00 accs: 0.35\n",
      "train, it: 3800 loss: 1.00 accs: 0.32\n",
      "valid, it: 4000 loss: 1.00 accs: 0.35\n",
      "\n",
      "train, it: 4000 loss: 1.00 accs: 0.34\n",
      "train, it: 4200 loss: 1.01 accs: 0.35\n",
      "train, it: 4400 loss: 1.00 accs: 0.33\n",
      "valid, it: 4500 loss: 1.00 accs: 0.36\n",
      "\n",
      "train, it: 4600 loss: 1.01 accs: 0.36\n",
      "train, it: 4800 loss: 1.00 accs: 0.38\n",
      "valid, it: 5000 loss: 1.00 accs: 0.36\n",
      "\n",
      "train, it: 5000 loss: 1.00 accs: 0.35\n"
     ]
    }
   ],
   "source": [
    "max_iter = 5000\n",
    "eval_every = 500\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "       # val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.premise,val_batch.hypothesis)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(output['out'])\n",
    "            #print(val_batch.label)\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "           # print(val_batch.batch_size)\n",
    "            \n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batch.premise,batch.hypothesis)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
