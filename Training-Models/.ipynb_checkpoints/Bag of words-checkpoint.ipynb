{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3a3220bf-9b31-4e8e-a6bd-56dceafddc64' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '3a3220bf-9b31-4e8e-a6bd-56dceafddc64' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.16.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.16.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.16.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"3a3220bf-9b31-4e8e-a6bd-56dceafddc64\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax, relu, tanh\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train_set, validation_set, _ = datasets.SST.splits(TEXT,\n",
    "                                                    LABEL,\n",
    "                                                    fine_grained=False,\n",
    "                                                    train_subtrees=True,\n",
    "                                                    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x000002603D074F60>, 'label': <torchtext.data.field.Field object at 0x000002603D0740F0>}\n",
      "len(train) 98794\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n",
      "\n",
      "Example 2 {'text': ['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train_set.fields)\n",
    "print('len(train)', len(train_set))\n",
    "print('vars(train[0])', vars(train_set[0]))\n",
    "print()\n",
    "print('Example 2', vars(train_set[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 18005\n",
      "TEXT.vocab.vectors.size() torch.Size([18005, 300])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "#url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "TEXT.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "#TEXT.build_vocab(train_set, vectors=Vectors('glove.840B.300d.txt',url = url))\n",
    "LABEL.build_vocab(train_set)\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3])\n",
      "tensor([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, _ = data.BucketIterator.splits(\n",
    "    (train_set, validation_set, _), batch_size=3)\n",
    "\n",
    "\n",
    "# print batch information\n",
    "#batch = next(iter(train_iter))\n",
    "batchsst = next(iter(train_iter))\n",
    "print(batchsst.text.size())\n",
    "print(batchsst.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWNet(\n",
      "  (embeddings): Embedding(18005, 300)\n",
      "  (input): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_dim = 200\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "# build the BoW model\n",
    "class BoWNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BoWNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        # add hidden layers\n",
    "        # YOUR CODE HERE!\n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = True)\n",
    "        \n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = True)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias= True)\n",
    "        self.l_3 = Linear(in_features= con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = True)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "        x = self.embeddings(x) # (bs,len,300)\n",
    "        #y = self.embeddings(y)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        #y = self.drop(y)\n",
    "        \n",
    "        #sum_x =  # (bs,300) \n",
    "\n",
    "        \n",
    "        sum_x = torch.sum(x,0)\n",
    "        #sum_y = torch.sum(y,0)\n",
    "        \n",
    "        \n",
    "        #tanh # (bs,100)\n",
    "        \n",
    "        sum_x = torch.tanh(self.input(sum_x))\n",
    "        #sum_y = torch.tanh(self.input(sum_y))\n",
    "    \n",
    "        #z = torch.cat((sum_x,sum_y),1)\n",
    "        z = sum_x\n",
    "        \n",
    "        z = torch.tanh(self.l_1(z))     \n",
    "        #z = torch.tanh(self.l_2(z))\n",
    "        #z = torch.tanh(self.l_3(z))\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(z), dim = 1)\n",
    "        return out\n",
    "\n",
    "net = BoWNet()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sentences(batch):\n",
    "    \"\"\"    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [\" \".join([TEXT.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.text[:,i])])\n",
    "            for i in range(batch.text.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch: torchtext.data.batch.Batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    [str]\n",
    "    \"\"\"\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derp = next(iter(val_iter))\n",
    "#print(derp.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING BOW WITH SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid, it: 0 loss: 1.10 accs: 0.29\n",
      "\n",
      "train, it: 0 loss: 1.09 accs: 0.67\n",
      "valid, it: 100 loss: 0.94 accs: 0.55\n",
      "\n",
      "train, it: 100 loss: 1.00 accs: 0.51\n",
      "valid, it: 200 loss: 0.90 accs: 0.65\n",
      "\n",
      "train, it: 200 loss: 0.94 accs: 0.57\n",
      "valid, it: 300 loss: 0.90 accs: 0.63\n",
      "\n",
      "train, it: 300 loss: 0.93 accs: 0.60\n",
      "valid, it: 400 loss: 0.91 accs: 0.62\n",
      "\n",
      "train, it: 400 loss: 0.91 accs: 0.63\n",
      "valid, it: 500 loss: 0.93 accs: 0.60\n",
      "\n",
      "train, it: 500 loss: 0.88 accs: 0.66\n",
      "valid, it: 600 loss: 0.89 accs: 0.65\n",
      "\n",
      "train, it: 600 loss: 0.87 accs: 0.66\n",
      "valid, it: 700 loss: 0.88 accs: 0.65\n",
      "\n",
      "train, it: 700 loss: 0.93 accs: 0.59\n",
      "valid, it: 800 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 800 loss: 0.87 accs: 0.69\n",
      "valid, it: 900 loss: 0.88 accs: 0.66\n",
      "\n",
      "train, it: 900 loss: 0.91 accs: 0.65\n",
      "valid, it: 1000 loss: 0.95 accs: 0.60\n",
      "\n",
      "train, it: 1000 loss: 0.85 accs: 0.70\n",
      "valid, it: 1100 loss: 0.98 accs: 0.56\n",
      "\n",
      "train, it: 1100 loss: 0.92 accs: 0.61\n",
      "valid, it: 1200 loss: 0.87 accs: 0.66\n",
      "\n",
      "train, it: 1200 loss: 0.88 accs: 0.67\n",
      "valid, it: 1300 loss: 0.88 accs: 0.65\n",
      "\n",
      "train, it: 1300 loss: 0.88 accs: 0.65\n",
      "valid, it: 1400 loss: 1.02 accs: 0.52\n",
      "\n",
      "train, it: 1400 loss: 0.90 accs: 0.65\n",
      "valid, it: 1500 loss: 0.94 accs: 0.59\n",
      "\n",
      "train, it: 1500 loss: 0.89 accs: 0.63\n",
      "valid, it: 1600 loss: 0.88 accs: 0.66\n",
      "\n",
      "train, it: 1600 loss: 0.87 accs: 0.67\n",
      "valid, it: 1700 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 1700 loss: 0.88 accs: 0.65\n",
      "valid, it: 1800 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 1800 loss: 0.90 accs: 0.65\n",
      "valid, it: 1900 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 1900 loss: 0.84 accs: 0.70\n",
      "valid, it: 2000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 2000 loss: 0.90 accs: 0.64\n",
      "valid, it: 2100 loss: 0.88 accs: 0.65\n",
      "\n",
      "train, it: 2100 loss: 0.84 accs: 0.71\n",
      "valid, it: 2200 loss: 0.86 accs: 0.69\n",
      "\n",
      "train, it: 2200 loss: 0.87 accs: 0.65\n",
      "valid, it: 2300 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 2300 loss: 0.83 accs: 0.72\n",
      "valid, it: 2400 loss: 0.87 accs: 0.68\n",
      "\n",
      "train, it: 2400 loss: 0.87 accs: 0.67\n",
      "valid, it: 2500 loss: 0.89 accs: 0.66\n",
      "\n",
      "train, it: 2500 loss: 0.85 accs: 0.70\n",
      "valid, it: 2600 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 2600 loss: 0.86 accs: 0.68\n",
      "valid, it: 2700 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 2700 loss: 0.89 accs: 0.64\n",
      "valid, it: 2800 loss: 0.84 accs: 0.69\n",
      "\n",
      "train, it: 2800 loss: 0.81 accs: 0.72\n",
      "valid, it: 2900 loss: 0.84 accs: 0.69\n",
      "\n",
      "train, it: 2900 loss: 0.81 accs: 0.72\n",
      "valid, it: 3000 loss: 0.89 accs: 0.65\n",
      "\n",
      "train, it: 3000 loss: 0.84 accs: 0.71\n",
      "valid, it: 3100 loss: 0.84 accs: 0.69\n",
      "\n",
      "train, it: 3100 loss: 0.84 accs: 0.70\n",
      "valid, it: 3200 loss: 0.86 accs: 0.67\n",
      "\n",
      "train, it: 3200 loss: 0.87 accs: 0.66\n",
      "valid, it: 3300 loss: 0.85 accs: 0.68\n",
      "\n",
      "train, it: 3300 loss: 0.86 accs: 0.68\n",
      "valid, it: 3400 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 3400 loss: 0.86 accs: 0.66\n",
      "valid, it: 3500 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 3500 loss: 0.81 accs: 0.74\n",
      "valid, it: 3600 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 3600 loss: 0.85 accs: 0.69\n",
      "valid, it: 3700 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 3700 loss: 0.84 accs: 0.70\n",
      "valid, it: 3800 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 3800 loss: 0.83 accs: 0.70\n",
      "valid, it: 3900 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 3900 loss: 0.85 accs: 0.69\n",
      "valid, it: 4000 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 4000 loss: 0.82 accs: 0.73\n",
      "valid, it: 4100 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 4100 loss: 0.82 accs: 0.73\n",
      "valid, it: 4200 loss: 0.84 accs: 0.69\n",
      "\n",
      "train, it: 4200 loss: 0.84 accs: 0.69\n",
      "valid, it: 4300 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 4300 loss: 0.86 accs: 0.68\n",
      "valid, it: 4400 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 4400 loss: 0.84 accs: 0.72\n",
      "valid, it: 4500 loss: 0.85 accs: 0.68\n",
      "\n",
      "train, it: 4500 loss: 0.82 accs: 0.71\n",
      "valid, it: 4600 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 4600 loss: 0.85 accs: 0.69\n",
      "valid, it: 4700 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 4700 loss: 0.85 accs: 0.69\n",
      "valid, it: 4800 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 4800 loss: 0.88 accs: 0.66\n",
      "valid, it: 4900 loss: 0.90 accs: 0.64\n",
      "\n",
      "train, it: 4900 loss: 0.88 accs: 0.68\n",
      "valid, it: 5000 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 5000 loss: 0.87 accs: 0.66\n",
      "valid, it: 5100 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 5100 loss: 0.85 accs: 0.69\n",
      "valid, it: 5200 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 5200 loss: 0.83 accs: 0.70\n",
      "valid, it: 5300 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 5300 loss: 0.81 accs: 0.74\n",
      "valid, it: 5400 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 5400 loss: 0.83 accs: 0.73\n",
      "valid, it: 5500 loss: 0.84 accs: 0.71\n",
      "\n",
      "train, it: 5500 loss: 0.90 accs: 0.63\n",
      "valid, it: 5600 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 5600 loss: 0.85 accs: 0.69\n",
      "valid, it: 5700 loss: 0.85 accs: 0.70\n",
      "\n",
      "train, it: 5700 loss: 0.87 accs: 0.67\n",
      "valid, it: 5800 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 5800 loss: 0.81 accs: 0.73\n",
      "valid, it: 5900 loss: 0.84 accs: 0.71\n",
      "\n",
      "train, it: 5900 loss: 0.81 accs: 0.72\n",
      "valid, it: 6000 loss: 0.86 accs: 0.69\n",
      "\n",
      "train, it: 6000 loss: 0.81 accs: 0.73\n",
      "valid, it: 6100 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 6100 loss: 0.82 accs: 0.72\n",
      "valid, it: 6200 loss: 0.86 accs: 0.69\n",
      "\n",
      "train, it: 6200 loss: 0.84 accs: 0.71\n",
      "valid, it: 6300 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 6300 loss: 0.80 accs: 0.75\n",
      "valid, it: 6400 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 6400 loss: 0.83 accs: 0.72\n",
      "valid, it: 6500 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 6500 loss: 0.77 accs: 0.77\n",
      "valid, it: 6600 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 6600 loss: 0.83 accs: 0.72\n",
      "valid, it: 6700 loss: 0.84 accs: 0.71\n",
      "\n",
      "train, it: 6700 loss: 0.83 accs: 0.72\n",
      "valid, it: 6800 loss: 0.84 accs: 0.71\n",
      "\n",
      "train, it: 6800 loss: 0.80 accs: 0.74\n",
      "valid, it: 6900 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 6900 loss: 0.79 accs: 0.76\n",
      "valid, it: 7000 loss: 0.81 accs: 0.73\n",
      "\n",
      "train, it: 7000 loss: 0.78 accs: 0.77\n",
      "valid, it: 7100 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 7100 loss: 0.77 accs: 0.78\n",
      "valid, it: 7200 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 7200 loss: 0.80 accs: 0.73\n",
      "valid, it: 7300 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 7300 loss: 0.83 accs: 0.70\n",
      "valid, it: 7400 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 7400 loss: 0.81 accs: 0.73\n",
      "valid, it: 7500 loss: 0.81 accs: 0.74\n",
      "\n",
      "train, it: 7500 loss: 0.81 accs: 0.73\n",
      "valid, it: 7600 loss: 0.81 accs: 0.74\n",
      "\n",
      "train, it: 7600 loss: 0.82 accs: 0.73\n",
      "valid, it: 7700 loss: 0.80 accs: 0.75\n",
      "\n",
      "train, it: 7700 loss: 0.79 accs: 0.75\n",
      "valid, it: 7800 loss: 0.81 accs: 0.74\n",
      "\n",
      "train, it: 7800 loss: 0.77 accs: 0.78\n",
      "valid, it: 7900 loss: 0.80 accs: 0.75\n",
      "\n",
      "train, it: 7900 loss: 0.85 accs: 0.70\n",
      "valid, it: 8000 loss: 0.81 accs: 0.74\n",
      "\n",
      "train, it: 8000 loss: 0.81 accs: 0.73\n",
      "valid, it: 8100 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 8100 loss: 0.86 accs: 0.68\n",
      "valid, it: 8200 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 8200 loss: 0.83 accs: 0.72\n",
      "valid, it: 8300 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 8300 loss: 0.79 accs: 0.73\n",
      "valid, it: 8400 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 8400 loss: 0.81 accs: 0.73\n",
      "valid, it: 8500 loss: 0.82 accs: 0.72\n",
      "\n",
      "train, it: 8500 loss: 0.82 accs: 0.72\n",
      "valid, it: 8600 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 8600 loss: 0.83 accs: 0.71\n",
      "valid, it: 8700 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 8700 loss: 0.83 accs: 0.72\n",
      "valid, it: 8800 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 8800 loss: 0.78 accs: 0.75\n",
      "valid, it: 8900 loss: 0.83 accs: 0.72\n",
      "\n",
      "train, it: 8900 loss: 0.79 accs: 0.74\n",
      "valid, it: 9000 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 9000 loss: 0.80 accs: 0.75\n",
      "valid, it: 9100 loss: 0.82 accs: 0.73\n",
      "\n",
      "train, it: 9100 loss: 0.80 accs: 0.74\n",
      "valid, it: 9200 loss: 0.83 accs: 0.73\n",
      "\n",
      "train, it: 9200 loss: 0.79 accs: 0.76\n",
      "valid, it: 9300 loss: 0.81 accs: 0.73\n",
      "\n",
      "train, it: 9300 loss: 0.79 accs: 0.76\n",
      "valid, it: 9400 loss: 0.81 accs: 0.74\n",
      "\n",
      "train, it: 9400 loss: 0.84 accs: 0.70\n",
      "valid, it: 9500 loss: 0.80 accs: 0.75\n",
      "\n",
      "train, it: 9500 loss: 0.80 accs: 0.73\n",
      "valid, it: 9600 loss: 0.81 accs: 0.73\n",
      "\n",
      "train, it: 9600 loss: 0.80 accs: 0.74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-202b8c490ade>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mbatch_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iter = 3000\n",
    "eval_every = 100\n",
    "log_every = 100\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batchsst in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.text)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(val_batch.label.size())\n",
    "          #  print(val_batch.label.size())\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            #print(val_batch.batch_size)\n",
    "       #     for key, _val in output.items():\n",
    "       #         if key not in val_meta:\n",
    "       #             val_meta[key] = []\n",
    "       #         val_meta[key].append(get_numpy(_val)) \n",
    "       #     val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "       #     val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "       #     val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "       # for key, _val in val_meta.items():\n",
    "       #     val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batchsst.text)\n",
    "    batch_loss = criterion(output['out'], batchsst.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batchsst.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run test on SNLI...\n",
      "Fields: {'premise': <torchtext.datasets.nli.ParsedTextField object at 0x000001F1FD06D710>, 'premise_transitions': <torchtext.datasets.nli.ShiftReduceField object at 0x000001F1FD06DE10>, 'hypothesis': <torchtext.datasets.nli.ParsedTextField object at 0x000001F1FD06D710>, 'hypothesis_transitions': <torchtext.datasets.nli.ShiftReduceField object at 0x000001F1FD06DE10>, 'label': <torchtext.data.field.LabelField object at 0x000001F1FD06DD30>}\n",
      "Number of examples:\n",
      " 549367\n",
      "First Example instance:\n",
      " {'premise': ['A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'premise_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'hypothesis': ['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'hypothesis_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'label': 'neutral'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4a977b3f72d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#batch = next(iter(train_iter))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36msplits\u001b[1;34m(cls, datasets, batch_sizes, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "print(\"Run test on SNLI...\")\n",
    "TEXT = datasets.nli.ParsedTextField()\n",
    "LABEL = data.LabelField()\n",
    "TREE = datasets.nli.ShiftReduceField()\n",
    "\n",
    "train, val, test = datasets.SNLI.splits(TEXT, LABEL, TREE)\n",
    "\n",
    "print(\"Fields:\", train.fields)\n",
    "print(\"Number of examples:\\n\", len(train))\n",
    "print(\"First Example instance:\\n\", vars(train[0]))\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.build_vocab(train, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "#TEXT.build_vocab(train,vectors=GloVe[name='840B',dim='300'])\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test))\n",
    "\n",
    "#batch = next(iter(train_iter))\n",
    "#print(\"Numericalize premises:\\n\", batch.premise)\n",
    "#print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "#print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "print(\"Test iters function\")\n",
    "train_iter, val_iter, test_iter = datasets.SNLI.iters(batch_size=128, trees=True)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Numericalize premises:\\n\", batch.premise)\n",
    "print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "print(\"Entailment labels:\\n\", batch.label)\n",
    "\n",
    "#val_iter_set = next(iter(val_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(train)\n",
    "#LABEL.build_vocab(train)\n",
    "print(TEXT.vocab.vectors.size()[0])\n",
    "print(len(LABEL.vocab.itos))\n",
    "#print(vars(TEXT.vocab))\n",
    "#print(vars(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias =True)\n",
    "\n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                          out_features=con_dim,\n",
    "                          bias=True)\n",
    "\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=True)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias =True)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "\n",
    "        x = self.embeddings(x[0])\n",
    "        y = self.embeddings(y[0])\n",
    "        \n",
    "        #x = torch.cat((x,y),2)\n",
    "        #x = self.drop(x)\n",
    "        #y = self.drop(y)\n",
    "        # mean embeddings, this is the bag of words trick\n",
    "        \n",
    "        \n",
    "        #print(x.size())\n",
    "        x = torch.sum(x, dim=0)\n",
    "        #print(x.size())\n",
    "        \n",
    "        y = torch.sum(y, dim=0)\n",
    "        \n",
    "        x = torch.tanh(self.input(x))\n",
    "        y = torch.tanh(self.input(y))\n",
    "        \n",
    "        x = torch.cat((x,y),1)\n",
    "        \n",
    "        # add hidden layers\n",
    "        # YOUR CODE HERE!\n",
    "        out['l1_activations'] = x = torch.tanh(self.l_1(x))\n",
    "        x = torch.tanh(self.l_2(x))\n",
    "        x = torch.tanh(self.l_2(x))\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter = 5000\n",
    "eval_every = 500\n",
    "log_every = 200\n",
    "\n",
    "# will be updated while iterating\n",
    "#tsne_plot = show(p, notebook_handle=True)\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "       # val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.premise,val_batch.hypothesis)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            #print(output['out'])\n",
    "            #print(val_batch.label)\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "           # print(val_batch.batch_size)\n",
    "            \n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        #update_plot(val_meta, 'bow', tsne_plot)\n",
    "        \n",
    "        net.train()\n",
    "    \n",
    "    output = net(batch.premise,batch.hypothesis)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
