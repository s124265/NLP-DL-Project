{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4RgA5rpx4tN"
   },
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 124982,
     "status": "ok",
     "timestamp": 1546465829136,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "299BGbZjx7yD",
    "outputId": "b4250ebd-e226-4abc-f998-c12be8223b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
      "Collecting torchtext\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/bc/b28b9efb4653c03e597ed207264eea45862b5260f48e9f010b5068d64db1/torchtext-0.3.1-py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.14.6)\n",
      "Collecting torch (from torchtext)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 591.8MB 23kB/s \n",
      "tcmalloc: large alloc 1073750016 bytes == 0x61d5a000 @  0x7f08e358d2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.11.29)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
      "Installing collected packages: torch, torchtext\n",
      "Successfully installed torch-1.0.0 torchtext-0.3.1\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.11.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.5.3)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.10)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.14.6)\n",
      "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.0.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (18.0)\n",
      "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.5.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.0->bokeh) (0.46)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install torchtext\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "!pip install bokeh\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 44253,
     "status": "ok",
     "timestamp": 1546465830736,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "fR-zMbrNx4tQ",
    "outputId": "8b6ee3ae-a9dd-48ed-c46b-4de8345ce687"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax, relu, tanh\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pKU7oTw4x4tW"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCKqJycyx4tZ"
   },
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51286,
     "status": "ok",
     "timestamp": 1546465840462,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "EkTytmxRx4ta",
    "outputId": "1df4bf40-cf93-4e1b-bae4-9bbd5454e223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading trainDevTestTrees_PTB.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:00<00:00, 861kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(sequential=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "train_set, validation_set, _ = datasets.SST.splits(TEXT,\n",
    "                                                    LABEL,\n",
    "                                                    fine_grained=False,\n",
    "                                                    train_subtrees=True,\n",
    "                                                    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49252,
     "status": "ok",
     "timestamp": 1546262060556,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "HxcPz4vJx4te",
    "outputId": "93640fb0-8058-4714-b5bf-1aeae3562f65",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f9109cadbe0>, 'label': <torchtext.data.field.Field object at 0x7f9109cadc18>}\n",
      "len(train) 98794\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n",
      "\n",
      "Example 2 {'text': ['The', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'The', 'Lord', 'of', 'the', 'Rings', \"''\", 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer\\\\/director', 'Peter', 'Jackson', \"'s\", 'expanded', 'vision', 'of', 'J.R.R.', 'Tolkien', \"'s\", 'Middle-earth', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train_set.fields)\n",
    "print('len(train)', len(train_set))\n",
    "print('vars(train[0])', vars(train_set[0]))\n",
    "print()\n",
    "print('Example 2', vars(train_set[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 635834,
     "status": "ok",
     "timestamp": 1546421435194,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "kMXsjuq-x4ti",
    "outputId": "1bf2409e-80eb-4407-fe83-75f4789d91e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.840B.300d.zip: 2.18GB [00:55, 39.6MB/s]                            \n",
      "100%|█████████▉| 2195617/2196017 [05:31<00:00, 7882.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 18003\n",
      "TEXT.vocab.vectors.size() torch.Size([18003, 300])\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "#url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "#TEXT.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "TEXT.build_vocab(train_set,vectors=GloVe(name='840B',dim='300'))\n",
    "LABEL.build_vocab(train_set)\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1546421458503,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "iDChimMdx4tm",
    "outputId": "ef67cfd4-00a4-4755-d5e0-1783ff016d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 32])\n",
      "tensor([2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, _ = data.BucketIterator.splits(\n",
    "    (train_set, validation_set, _), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1546421458871,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "UwgdThFMx4tq",
    "outputId": "894a6790-2e7b-4f70-d5c3-0846f53cc299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoWNet(\n",
      "  (embeddings): Embedding(18003, 300)\n",
      "  (input): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "input_dim = 200\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "# build the BoW model\n",
    "class BoWNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BoWNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias = True)\n",
    "        \n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias = True)\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias= True)\n",
    "        self.l_3 = Linear(in_features= con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias = True)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        \n",
    "        x = self.embeddings(x) # (bs,len,300)\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        \n",
    "        #sum_x =  # (bs,300) \n",
    "\n",
    "        \n",
    "        sum_x = torch.sum(x,0)\n",
    "        \n",
    "        \n",
    "        #tanh # (bs,100)\n",
    "        \n",
    "        sum_x = torch.tanh(self.input(sum_x))\n",
    "        z = sum_x\n",
    "        \n",
    "        z = torch.tanh(self.l_1(z))     \n",
    "        z = torch.tanh(self.l_2(z))\n",
    "        z = torch.tanh(self.l_3(z))\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(z), dim = 1)\n",
    "        return out\n",
    "\n",
    "net = BoWNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "S96UHc2Ax4tu"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001,amsgrad=True,weight_decay=0.00001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DNmPqHCqx4tz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M5NRTtgBx4t1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkNYIXxVx4t5"
   },
   "source": [
    "## TRAINING BOW WITH SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2554
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5072704,
     "status": "ok",
     "timestamp": 1546426540228,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "Bh5fF9jLx4t6",
    "outputId": "fa1eac65-74e8-41ea-fed2-2537529b9af2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH NR: 1\n",
      "valid, it: 0 loss: 1.10 accs: 0.16\n",
      "\n",
      "train, it: 0 loss: 1.10 accs: 0.31\n",
      "train, it: 200 loss: 0.77 accs: 0.78\n",
      "train, it: 400 loss: 0.74 accs: 0.81\n",
      "train, it: 600 loss: 0.74 accs: 0.81\n",
      "train, it: 800 loss: 0.73 accs: 0.81\n",
      "valid, it: 1000 loss: 0.78 accs: 0.76\n",
      "\n",
      "train, it: 1000 loss: 0.77 accs: 0.78\n",
      "train, it: 1200 loss: 0.74 accs: 0.81\n",
      "train, it: 1400 loss: 0.75 accs: 0.80\n",
      "train, it: 1600 loss: 0.73 accs: 0.81\n",
      "train, it: 1800 loss: 0.74 accs: 0.81\n",
      "valid, it: 2000 loss: 0.80 accs: 0.75\n",
      "\n",
      "train, it: 2000 loss: 0.76 accs: 0.79\n",
      "train, it: 2200 loss: 0.76 accs: 0.79\n",
      "train, it: 2400 loss: 0.73 accs: 0.82\n",
      "train, it: 2600 loss: 0.75 accs: 0.80\n",
      "train, it: 2800 loss: 0.73 accs: 0.82\n",
      "valid, it: 3000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 3000 loss: 0.74 accs: 0.81\n",
      "EPOCH NR: 2\n",
      "valid, it: 0 loss: 0.78 accs: 0.76\n",
      "\n",
      "train, it: 0 loss: 0.72 accs: 0.83\n",
      "train, it: 200 loss: 0.72 accs: 0.82\n",
      "train, it: 400 loss: 0.72 accs: 0.83\n",
      "train, it: 600 loss: 0.72 accs: 0.83\n",
      "train, it: 800 loss: 0.71 accs: 0.84\n",
      "valid, it: 1000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 1000 loss: 0.72 accs: 0.83\n",
      "train, it: 1200 loss: 0.71 accs: 0.84\n",
      "train, it: 1400 loss: 0.71 accs: 0.84\n",
      "train, it: 1600 loss: 0.72 accs: 0.83\n",
      "train, it: 1800 loss: 0.72 accs: 0.83\n",
      "valid, it: 2000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 2000 loss: 0.71 accs: 0.84\n",
      "train, it: 2200 loss: 0.72 accs: 0.83\n",
      "train, it: 2400 loss: 0.73 accs: 0.82\n",
      "train, it: 2600 loss: 0.71 accs: 0.84\n",
      "train, it: 2800 loss: 0.71 accs: 0.84\n",
      "valid, it: 3000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 3000 loss: 0.70 accs: 0.85\n",
      "EPOCH NR: 3\n",
      "valid, it: 0 loss: 0.76 accs: 0.79\n",
      "\n",
      "train, it: 0 loss: 0.71 accs: 0.84\n",
      "train, it: 200 loss: 0.69 accs: 0.86\n",
      "train, it: 400 loss: 0.68 accs: 0.87\n",
      "train, it: 600 loss: 0.70 accs: 0.85\n",
      "train, it: 800 loss: 0.70 accs: 0.85\n",
      "valid, it: 1000 loss: 0.79 accs: 0.76\n",
      "\n",
      "train, it: 1000 loss: 0.69 accs: 0.86\n",
      "train, it: 1200 loss: 0.71 accs: 0.84\n",
      "train, it: 1400 loss: 0.70 accs: 0.84\n",
      "train, it: 1600 loss: 0.71 accs: 0.84\n",
      "train, it: 1800 loss: 0.69 accs: 0.86\n",
      "valid, it: 2000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 2000 loss: 0.68 accs: 0.87\n",
      "train, it: 2200 loss: 0.70 accs: 0.85\n",
      "train, it: 2400 loss: 0.69 accs: 0.86\n",
      "train, it: 2600 loss: 0.72 accs: 0.83\n",
      "train, it: 2800 loss: 0.70 accs: 0.85\n",
      "valid, it: 3000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 3000 loss: 0.70 accs: 0.85\n",
      "EPOCH NR: 4\n",
      "valid, it: 0 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 0 loss: 0.69 accs: 0.86\n",
      "train, it: 200 loss: 0.69 accs: 0.86\n",
      "train, it: 400 loss: 0.69 accs: 0.86\n",
      "train, it: 600 loss: 0.69 accs: 0.86\n",
      "train, it: 800 loss: 0.68 accs: 0.86\n",
      "valid, it: 1000 loss: 0.76 accs: 0.79\n",
      "\n",
      "train, it: 1000 loss: 0.69 accs: 0.86\n",
      "train, it: 1200 loss: 0.69 accs: 0.86\n",
      "train, it: 1400 loss: 0.69 accs: 0.86\n",
      "train, it: 1600 loss: 0.69 accs: 0.86\n",
      "train, it: 1800 loss: 0.69 accs: 0.86\n",
      "valid, it: 2000 loss: 0.75 accs: 0.81\n",
      "\n",
      "train, it: 2000 loss: 0.69 accs: 0.86\n",
      "train, it: 2200 loss: 0.69 accs: 0.86\n",
      "train, it: 2400 loss: 0.68 accs: 0.87\n",
      "train, it: 2600 loss: 0.68 accs: 0.87\n",
      "train, it: 2800 loss: 0.69 accs: 0.86\n",
      "valid, it: 3000 loss: 0.75 accs: 0.80\n",
      "\n",
      "train, it: 3000 loss: 0.68 accs: 0.87\n",
      "EPOCH NR: 5\n",
      "valid, it: 0 loss: 0.75 accs: 0.80\n",
      "\n",
      "train, it: 0 loss: 0.68 accs: 0.87\n",
      "train, it: 200 loss: 0.68 accs: 0.87\n",
      "train, it: 400 loss: 0.67 accs: 0.88\n",
      "train, it: 600 loss: 0.68 accs: 0.87\n",
      "train, it: 800 loss: 0.67 accs: 0.88\n",
      "valid, it: 1000 loss: 0.74 accs: 0.81\n",
      "\n",
      "train, it: 1000 loss: 0.67 accs: 0.88\n",
      "train, it: 1200 loss: 0.68 accs: 0.87\n",
      "train, it: 1400 loss: 0.68 accs: 0.87\n",
      "train, it: 1600 loss: 0.68 accs: 0.87\n",
      "train, it: 1800 loss: 0.67 accs: 0.88\n",
      "valid, it: 2000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 2000 loss: 0.71 accs: 0.84\n",
      "train, it: 2200 loss: 0.69 accs: 0.86\n",
      "train, it: 2400 loss: 0.69 accs: 0.86\n",
      "train, it: 2600 loss: 0.68 accs: 0.87\n",
      "train, it: 2800 loss: 0.70 accs: 0.85\n",
      "valid, it: 3000 loss: 0.78 accs: 0.77\n",
      "\n",
      "train, it: 3000 loss: 0.68 accs: 0.87\n",
      "EPOCH NR: 6\n",
      "valid, it: 0 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 0 loss: 0.69 accs: 0.86\n",
      "train, it: 200 loss: 0.69 accs: 0.86\n",
      "train, it: 400 loss: 0.68 accs: 0.87\n",
      "train, it: 600 loss: 0.66 accs: 0.89\n",
      "train, it: 800 loss: 0.67 accs: 0.88\n",
      "valid, it: 1000 loss: 0.77 accs: 0.78\n",
      "\n",
      "train, it: 1000 loss: 0.67 accs: 0.88\n",
      "train, it: 1200 loss: 0.67 accs: 0.88\n",
      "train, it: 1400 loss: 0.67 accs: 0.88\n",
      "train, it: 1600 loss: 0.67 accs: 0.88\n",
      "train, it: 1800 loss: 0.67 accs: 0.88\n",
      "valid, it: 2000 loss: 0.76 accs: 0.79\n",
      "\n",
      "train, it: 2000 loss: 0.67 accs: 0.88\n",
      "train, it: 2200 loss: 0.69 accs: 0.86\n",
      "train, it: 2400 loss: 0.68 accs: 0.87\n",
      "train, it: 2600 loss: 0.68 accs: 0.87\n",
      "train, it: 2800 loss: 0.67 accs: 0.88\n",
      "valid, it: 3000 loss: 0.76 accs: 0.79\n",
      "\n",
      "train, it: 3000 loss: 0.68 accs: 0.87\n",
      "Maximum validation accuracy: 0.8107798\n"
     ]
    }
   ],
   "source": [
    "max_iter = 3000\n",
    "eval_every = 1000\n",
    "log_every = 200\n",
    "\n",
    "train_loss, train_accs, train_iter_list = [], [], []\n",
    "train_loss_list, train_accs_list = [],[]\n",
    "val_loss_list, val_accs_list, val_iter_list = [],[], []\n",
    "\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_idx = 0\n",
    "epochs = 1\n",
    "reached_max = False\n",
    "\n",
    "net.train()\n",
    "while reached_max == False:\n",
    "    print(\"EPOCH NR: \" + str(epochs))\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        if i % eval_every == 0:\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "            for val_batch in val_iter:\n",
    "                output = net(val_batch.text)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "                val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "                val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "                val_lengths += val_batch.batch_size\n",
    "            \n",
    "\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "        \n",
    "            val_loss_list.append(get_numpy(val_losses))\n",
    "            val_accs_list.append(get_numpy(val_accs))\n",
    "            val_iter_list.append(i)\n",
    "        \n",
    "            print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "          \n",
    "            net.train()\n",
    "    \n",
    "        output = net(batch.text)\n",
    "        batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "        train_loss.append(get_numpy(batch_loss))\n",
    "        train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    " \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(),max_norm=0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % log_every == 0:        \n",
    "            print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "            train_loss_list.append(np.mean(train_loss))\n",
    "            train_accs_list.append(np.mean(train_accs))    \n",
    "            train_iter_list.append(i)\n",
    "            train_loss, train_accs = [], []\n",
    "    if max(val_accs_list[max_acc_idx:len(val_accs_list)]) > max_acc:\n",
    "        max_acc = max(val_accs_list[max_acc_idx:len(val_accs_list)])\n",
    "        max_acc_idx = np.argmax(max(val_accs_list[max_acc_idx:len(val_accs_list)]))\n",
    "    else:\n",
    "        print(\"Maximum validation accuracy: \" + str(max_acc))\n",
    "        reached_max = True\n",
    "        break\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "L7q_JurSx4t9"
   },
   "outputs": [],
   "source": [
    "for i in range(int(len(train_iter_list)/(epochs)),len(train_iter_list)):\n",
    "    if train_iter_list[i] == 0:\n",
    "        train_iter_list[i] = train_iter_list[i-1]+1\n",
    "        continue\n",
    "    else:\n",
    "        train_iter_list[i] = train_iter_list[i-1] + log_every\n",
    "\n",
    "\n",
    "for i in range(int(len(val_iter_list)/epochs),len(val_iter_list)):\n",
    "    if val_iter_list[i] == 0:\n",
    "        val_iter_list[i] =val_iter_list[i-1]+1\n",
    "        continue\n",
    "    else:\n",
    "        val_iter_list[i] = val_iter_list[i-1] + eval_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5069801,
     "status": "ok",
     "timestamp": 1546426541233,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "CuGRtkTAx4uA",
    "outputId": "7bd5caf7-0d17-4ff2-c901-30c6055ef26c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAD4CAYAAAANWzs4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe519cc9f28>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_iter_list, train_loss_list, label='train_loss')\n",
    "plt.plot(val_iter_list, val_loss_list, label='valid_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_iter_list, train_accs_list, label='train_accs')\n",
    "plt.plot(val_iter_list, val_accs_list, label='valid_accs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ox2XnNQUH7MF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtqfXnORx4uE"
   },
   "source": [
    "## SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 95802,
     "status": "ok",
     "timestamp": 1546465913381,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "lVbycArIx4uE",
    "outputId": "3d808328-b6ef-42a9-e4e3-d778a562a49c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run test on SNLI...\n",
      "downloading snli_1.0.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:05<00:00, 15.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      ".vector_cache/wiki.simple.vec: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields: {'premise': <torchtext.datasets.nli.ParsedTextField object at 0x7fd73f66d5c0>, 'premise_transitions': <torchtext.datasets.nli.ShiftReduceField object at 0x7fd73f67b7f0>, 'hypothesis': <torchtext.datasets.nli.ParsedTextField object at 0x7fd73f66d5c0>, 'hypothesis_transitions': <torchtext.datasets.nli.ShiftReduceField object at 0x7fd73f67b7f0>, 'label': <torchtext.data.field.LabelField object at 0x7fd73f674cc0>}\n",
      "Number of examples:\n",
      " 549367\n",
      "First Example instance:\n",
      " {'premise': ['A', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'premise_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'hypothesis': ['A', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'hypothesis_transitions': ['shift', 'shift', 'reduce', 'shift', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'shift', 'shift', 'shift', 'reduce', 'reduce', 'reduce', 'reduce', 'shift', 'reduce', 'reduce'], 'label': 'neutral'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.simple.vec: 293MB [00:09, 30.1MB/s]                           \n",
      "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
      " 99%|█████████▉| 110275/111051 [00:12<00:00, 8448.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test iters function\n",
      "Numericalize premises:\n",
      " (tensor([[   4,   58,   14,  ...,   14,    4,   20],\n",
      "        [  46, 1792,   34,  ...,   25,    8,   48],\n",
      "        [  21, 2668,    6,  ...,   21,    5, 1599],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]]), tensor([18,  8, 11, 11,  9, 15, 42, 11, 14, 18, 12, 20, 14, 19, 12, 12, 11, 20,\n",
      "         8, 15, 20,  9, 16, 23, 11, 11, 17, 18, 13, 16, 20, 10, 14,  7, 17, 16,\n",
      "        17, 13, 11,  8, 21, 12, 17, 14, 13, 14, 12, 18, 10, 17, 12, 12, 17, 12,\n",
      "        15, 27,  8,  9, 11, 13, 10, 15, 19, 10]))\n",
      "Numericalize hypotheses:\n",
      " (tensor([[  4,   4,   4,  ...,   7,   4,  20],\n",
      "        [  8,   8,  34,  ...,  25,   8,  64],\n",
      "        [  6, 581,   6,  ...,   6,  21,  11],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]]), tensor([11,  7,  7,  5,  9,  9, 23,  8, 13,  7, 13,  6,  5, 15,  6,  9,  5,  5,\n",
      "         8,  8, 10,  5, 16, 17, 11,  9,  7,  7, 18,  8, 12, 12, 12,  6,  6, 13,\n",
      "        10, 10,  6,  8, 25,  9,  9, 14,  7, 14,  8, 10,  7,  8,  9, 10,  7, 12,\n",
      "        11, 11, 12,  9,  7, 13, 10,  6, 17, 10]))\n",
      "Entailment labels:\n",
      " tensor([1, 1, 1, 2, 1, 0, 1, 0, 0, 2, 2, 0, 2, 2, 1, 0, 0, 1, 1, 1, 2, 0, 2, 2,\n",
      "        1, 0, 0, 1, 2, 1, 2, 2, 0, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 2, 2, 0,\n",
      "        2, 1, 0, 1, 2, 1, 0, 2, 1, 1, 2, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Run test on SNLI...\")\n",
    "TEXT = datasets.nli.ParsedTextField()\n",
    "LABEL = data.LabelField()\n",
    "TREE = datasets.nli.ShiftReduceField()\n",
    "\n",
    "train, val, test = datasets.SNLI.splits(TEXT, LABEL, TREE)\n",
    "\n",
    "print(\"Fields:\", train.fields)\n",
    "print(\"Number of examples:\\n\", len(train))\n",
    "print(\"First Example instance:\\n\", vars(train[0]))\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "#TEXT.build_vocab(train, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "TEXT.build_vocab(train_set,vectors=GloVe(name='840B',dim='300'))\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits((train, val, test), batch_size=64)\n",
    "\n",
    "\n",
    "print(\"Test iters function\")\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(\"Numericalize premises:\\n\", batch.premise)\n",
    "print(\"Numericalize hypotheses:\\n\", batch.hypothesis)\n",
    "print(\"Entailment labels:\\n\", batch.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1546466091582,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "hQuL24WDx4uJ",
    "outputId": "0290effa-0423-42f3-e6d6-f41c7b86d32e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42392\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(TEXT.vocab.vectors.size()[0])\n",
    "print(len(LABEL.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1546466093720,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "gtQ01_4Fx4uO",
    "outputId": "b1dd7663-768e-40cf-a3b4-89fe7c49a11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embeddings): Embedding(42392, 300)\n",
      "  (input): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (l_1): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (l_3): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (drop): Dropout(p=0.2)\n",
      "  (l_out): Linear(in_features=200, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "con_dim = 200\n",
    "\n",
    "dropout_rate = 0.2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(TEXT.vocab.vectors)\n",
    "        \n",
    "        \n",
    "        self.input = Linear(in_features = embedding_dim,\n",
    "                             out_features = input_dim,\n",
    "                             bias =True)\n",
    "\n",
    "        self.l_1 = Linear(in_features=con_dim,\n",
    "                          out_features=con_dim,\n",
    "                          bias=True)\n",
    "\n",
    "        self.l_2 = Linear(in_features=con_dim,\n",
    "                           out_features=con_dim,\n",
    "                           bias=True)\n",
    "        self.l_3 = Linear(in_features=con_dim,\n",
    "                           out_features = con_dim,\n",
    "                           bias =True)\n",
    "        \n",
    "        self.drop = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "        \n",
    "        # output layer\n",
    "        self.l_out = Linear(in_features=con_dim,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        out = {}\n",
    "        # get embeddings\n",
    "\n",
    "        x = self.embeddings(x[0])\n",
    "        y = self.embeddings(y[0])\n",
    "        \n",
    "        x = self.drop(x)\n",
    "        y = self.drop(y)\n",
    "        \n",
    "        x = torch.sum(x, dim=0)\n",
    "        \n",
    "        y = torch.sum(y, dim=0)\n",
    "        \n",
    "        x = torch.tanh(self.input(x))\n",
    "        y = torch.tanh(self.input(y))\n",
    "        \n",
    "        x = torch.cat((x,y),1)\n",
    "\n",
    "        x = torch.tanh(self.l_1(x))\n",
    "        x = torch.tanh(self.l_2(x))\n",
    "        x = torch.tanh(self.l_2(x))\n",
    "\n",
    "        # Softmax\n",
    "        out['out'] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F8gW3HI9x4uS"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001,amsgrad=True,weight_decay=0.00001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3847
    },
    "colab_type": "code",
    "id": "BkEXWB3kx4uV",
    "outputId": "78d5a674-3068-4a60-979d-b7f2bb39b967",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH NR: 1\n",
      "valid, it: 0 loss: 1.10 accs: 0.33\n",
      "\n",
      "train, it: 0 loss: 1.10 accs: 0.38\n",
      "train, it: 200 loss: 1.01 accs: 0.51\n",
      "train, it: 400 loss: 0.96 accs: 0.57\n",
      "train, it: 600 loss: 0.95 accs: 0.58\n",
      "train, it: 800 loss: 0.95 accs: 0.59\n",
      "valid, it: 1000 loss: 0.95 accs: 0.58\n",
      "\n",
      "train, it: 1000 loss: 0.94 accs: 0.59\n",
      "train, it: 1200 loss: 0.94 accs: 0.59\n",
      "train, it: 1400 loss: 0.93 accs: 0.60\n",
      "train, it: 1600 loss: 0.93 accs: 0.60\n",
      "train, it: 1800 loss: 0.93 accs: 0.61\n",
      "valid, it: 2000 loss: 0.92 accs: 0.62\n",
      "\n",
      "train, it: 2000 loss: 0.93 accs: 0.61\n",
      "train, it: 2200 loss: 0.93 accs: 0.60\n",
      "train, it: 2400 loss: 0.92 accs: 0.61\n",
      "train, it: 2600 loss: 0.92 accs: 0.61\n",
      "train, it: 2800 loss: 0.92 accs: 0.62\n",
      "valid, it: 3000 loss: 0.91 accs: 0.62\n",
      "\n",
      "train, it: 3000 loss: 0.92 accs: 0.62\n",
      "train, it: 3200 loss: 0.91 accs: 0.62\n",
      "train, it: 3400 loss: 0.91 accs: 0.62\n",
      "train, it: 3600 loss: 0.91 accs: 0.62\n",
      "train, it: 3800 loss: 0.91 accs: 0.62\n",
      "valid, it: 4000 loss: 0.91 accs: 0.63\n",
      "\n",
      "train, it: 4000 loss: 0.91 accs: 0.63\n",
      "train, it: 4200 loss: 0.91 accs: 0.63\n",
      "train, it: 4400 loss: 0.90 accs: 0.63\n",
      "train, it: 4600 loss: 0.90 accs: 0.63\n",
      "train, it: 4800 loss: 0.89 accs: 0.64\n",
      "valid, it: 5000 loss: 0.90 accs: 0.63\n",
      "\n",
      "train, it: 5000 loss: 0.90 accs: 0.63\n",
      "train, it: 5200 loss: 0.89 accs: 0.65\n",
      "train, it: 5400 loss: 0.90 accs: 0.64\n",
      "train, it: 5600 loss: 0.89 accs: 0.64\n",
      "train, it: 5800 loss: 0.88 accs: 0.66\n",
      "valid, it: 6000 loss: 0.89 accs: 0.65\n",
      "\n",
      "train, it: 6000 loss: 0.89 accs: 0.65\n",
      "train, it: 6200 loss: 0.89 accs: 0.65\n",
      "train, it: 6400 loss: 0.88 accs: 0.65\n",
      "train, it: 6600 loss: 0.89 accs: 0.65\n",
      "train, it: 6800 loss: 0.88 accs: 0.66\n",
      "valid, it: 7000 loss: 0.88 accs: 0.66\n",
      "\n",
      "train, it: 7000 loss: 0.88 accs: 0.66\n",
      "train, it: 7200 loss: 0.88 accs: 0.66\n",
      "train, it: 7400 loss: 0.88 accs: 0.65\n",
      "train, it: 7600 loss: 0.88 accs: 0.65\n",
      "train, it: 7800 loss: 0.88 accs: 0.66\n",
      "valid, it: 8000 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 8000 loss: 0.88 accs: 0.66\n",
      "train, it: 8200 loss: 0.87 accs: 0.67\n",
      "train, it: 8400 loss: 0.88 accs: 0.65\n",
      "EPOCH NR: 2\n",
      "valid, it: 0 loss: 0.88 accs: 0.65\n",
      "\n",
      "train, it: 0 loss: 0.88 accs: 0.66\n",
      "train, it: 200 loss: 0.87 accs: 0.67\n",
      "train, it: 400 loss: 0.87 accs: 0.67\n",
      "train, it: 600 loss: 0.87 accs: 0.67\n",
      "train, it: 800 loss: 0.87 accs: 0.67\n",
      "valid, it: 1000 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 1000 loss: 0.86 accs: 0.68\n",
      "train, it: 1200 loss: 0.86 accs: 0.68\n",
      "train, it: 1400 loss: 0.87 accs: 0.67\n",
      "train, it: 1600 loss: 0.87 accs: 0.67\n",
      "train, it: 1800 loss: 0.87 accs: 0.67\n",
      "valid, it: 2000 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 2000 loss: 0.87 accs: 0.67\n",
      "train, it: 2200 loss: 0.87 accs: 0.67\n",
      "train, it: 2400 loss: 0.86 accs: 0.67\n",
      "train, it: 2600 loss: 0.86 accs: 0.68\n",
      "train, it: 2800 loss: 0.87 accs: 0.67\n",
      "valid, it: 3000 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 3000 loss: 0.87 accs: 0.67\n",
      "train, it: 3200 loss: 0.87 accs: 0.67\n",
      "train, it: 3400 loss: 0.86 accs: 0.68\n",
      "train, it: 3600 loss: 0.87 accs: 0.67\n",
      "train, it: 3800 loss: 0.86 accs: 0.68\n",
      "valid, it: 4000 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 4000 loss: 0.86 accs: 0.68\n",
      "train, it: 4200 loss: 0.87 accs: 0.67\n",
      "train, it: 4400 loss: 0.86 accs: 0.68\n",
      "train, it: 4600 loss: 0.86 accs: 0.67\n",
      "train, it: 4800 loss: 0.86 accs: 0.68\n",
      "valid, it: 5000 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 5000 loss: 0.86 accs: 0.68\n",
      "train, it: 5200 loss: 0.86 accs: 0.67\n",
      "train, it: 5400 loss: 0.86 accs: 0.68\n",
      "train, it: 5600 loss: 0.86 accs: 0.68\n",
      "train, it: 5800 loss: 0.86 accs: 0.68\n",
      "valid, it: 6000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 6000 loss: 0.86 accs: 0.68\n",
      "train, it: 6200 loss: 0.86 accs: 0.68\n",
      "train, it: 6400 loss: 0.86 accs: 0.68\n",
      "train, it: 6600 loss: 0.86 accs: 0.68\n",
      "train, it: 6800 loss: 0.86 accs: 0.68\n",
      "valid, it: 7000 loss: 0.85 accs: 0.68\n",
      "\n",
      "train, it: 7000 loss: 0.86 accs: 0.68\n",
      "train, it: 7200 loss: 0.86 accs: 0.68\n",
      "train, it: 7400 loss: 0.86 accs: 0.68\n",
      "train, it: 7600 loss: 0.86 accs: 0.68\n",
      "train, it: 7800 loss: 0.86 accs: 0.68\n",
      "valid, it: 8000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 8000 loss: 0.86 accs: 0.68\n",
      "train, it: 8200 loss: 0.87 accs: 0.68\n",
      "train, it: 8400 loss: 0.86 accs: 0.68\n",
      "EPOCH NR: 3\n",
      "valid, it: 0 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 0 loss: 0.86 accs: 0.68\n",
      "train, it: 200 loss: 0.84 accs: 0.70\n",
      "train, it: 400 loss: 0.84 accs: 0.70\n",
      "train, it: 600 loss: 0.85 accs: 0.69\n",
      "train, it: 800 loss: 0.84 accs: 0.70\n",
      "valid, it: 1000 loss: 0.86 accs: 0.68\n",
      "\n",
      "train, it: 1000 loss: 0.84 accs: 0.70\n",
      "train, it: 1200 loss: 0.84 accs: 0.70\n",
      "train, it: 1400 loss: 0.85 accs: 0.69\n",
      "train, it: 1600 loss: 0.85 accs: 0.69\n",
      "train, it: 1800 loss: 0.85 accs: 0.69\n",
      "valid, it: 2000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 2000 loss: 0.85 accs: 0.69\n",
      "train, it: 2200 loss: 0.84 accs: 0.70\n",
      "train, it: 2400 loss: 0.85 accs: 0.68\n",
      "train, it: 2600 loss: 0.85 accs: 0.69\n",
      "train, it: 2800 loss: 0.85 accs: 0.69\n",
      "valid, it: 3000 loss: 0.87 accs: 0.67\n",
      "\n",
      "train, it: 3000 loss: 0.85 accs: 0.70\n",
      "train, it: 3200 loss: 0.85 accs: 0.69\n",
      "train, it: 3400 loss: 0.85 accs: 0.70\n",
      "train, it: 3600 loss: 0.85 accs: 0.69\n",
      "train, it: 3800 loss: 0.85 accs: 0.69\n",
      "valid, it: 4000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 4000 loss: 0.85 accs: 0.70\n",
      "train, it: 4200 loss: 0.85 accs: 0.69\n",
      "train, it: 4400 loss: 0.85 accs: 0.69\n",
      "train, it: 4600 loss: 0.85 accs: 0.69\n",
      "train, it: 4800 loss: 0.86 accs: 0.68\n",
      "valid, it: 5000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 5000 loss: 0.85 accs: 0.69\n",
      "train, it: 5200 loss: 0.85 accs: 0.69\n",
      "train, it: 5400 loss: 0.85 accs: 0.69\n",
      "train, it: 5600 loss: 0.84 accs: 0.70\n",
      "train, it: 5800 loss: 0.85 accs: 0.70\n",
      "valid, it: 6000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 6000 loss: 0.85 accs: 0.70\n",
      "train, it: 6200 loss: 0.85 accs: 0.69\n",
      "train, it: 6400 loss: 0.85 accs: 0.69\n",
      "train, it: 6600 loss: 0.85 accs: 0.69\n",
      "train, it: 6800 loss: 0.85 accs: 0.69\n",
      "valid, it: 7000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 7000 loss: 0.85 accs: 0.69\n",
      "train, it: 7200 loss: 0.85 accs: 0.69\n",
      "train, it: 7400 loss: 0.85 accs: 0.69\n",
      "train, it: 7600 loss: 0.85 accs: 0.69\n",
      "train, it: 7800 loss: 0.85 accs: 0.69\n",
      "valid, it: 8000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 8000 loss: 0.85 accs: 0.69\n",
      "train, it: 8200 loss: 0.85 accs: 0.69\n",
      "train, it: 8400 loss: 0.85 accs: 0.69\n",
      "EPOCH NR: 4\n",
      "valid, it: 0 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 0 loss: 0.84 accs: 0.70\n",
      "train, it: 200 loss: 0.83 accs: 0.71\n",
      "train, it: 400 loss: 0.84 accs: 0.70\n",
      "train, it: 600 loss: 0.84 accs: 0.70\n",
      "train, it: 800 loss: 0.83 accs: 0.71\n",
      "valid, it: 1000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 1000 loss: 0.84 accs: 0.71\n",
      "train, it: 1200 loss: 0.84 accs: 0.71\n",
      "train, it: 1400 loss: 0.83 accs: 0.71\n",
      "train, it: 1600 loss: 0.83 accs: 0.71\n",
      "train, it: 1800 loss: 0.84 accs: 0.71\n",
      "valid, it: 2000 loss: 0.85 accs: 0.69\n",
      "\n",
      "train, it: 2000 loss: 0.83 accs: 0.71\n",
      "train, it: 2200 loss: 0.84 accs: 0.70\n",
      "train, it: 2400 loss: 0.84 accs: 0.71\n",
      "train, it: 2600 loss: 0.83 accs: 0.71\n",
      "train, it: 2800 loss: 0.83 accs: 0.71\n",
      "valid, it: 3000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 3000 loss: 0.84 accs: 0.69\n",
      "train, it: 3200 loss: 0.84 accs: 0.70\n",
      "train, it: 3400 loss: 0.84 accs: 0.70\n",
      "train, it: 3600 loss: 0.84 accs: 0.71\n",
      "train, it: 3800 loss: 0.83 accs: 0.71\n",
      "valid, it: 4000 loss: 0.84 accs: 0.70\n",
      "\n",
      "train, it: 4000 loss: 0.85 accs: 0.69\n",
      "train, it: 4200 loss: 0.84 accs: 0.70\n",
      "train, it: 4400 loss: 0.84 accs: 0.71\n",
      "train, it: 4600 loss: 0.84 accs: 0.70\n",
      "train, it: 4800 loss: 0.84 accs: 0.71\n",
      "valid, it: 5000 loss: 0.83 accs: 0.71\n",
      "\n",
      "train, it: 5000 loss: 0.84 accs: 0.70\n",
      "train, it: 5200 loss: 0.83 accs: 0.71\n",
      "train, it: 5400 loss: 0.84 accs: 0.70\n",
      "train, it: 5600 loss: 0.83 accs: 0.72\n"
     ]
    }
   ],
   "source": [
    "max_iter = 3000\n",
    "eval_every = 1000\n",
    "log_every = 200\n",
    "\n",
    "train_loss, train_accs, train_iter_list = [], [], []\n",
    "train_loss_list, train_accs_list = [],[]\n",
    "val_loss_list, val_accs_list, val_iter_list = [],[], []\n",
    "\n",
    "\n",
    "max_acc = 0\n",
    "max_acc_idx = 0\n",
    "epochs = 1\n",
    "reached_max = False\n",
    "\n",
    "net.train()\n",
    "while reached_max == False:\n",
    "    print(\"EPOCH NR: \" + str(epochs))\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        if i % eval_every == 0:\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "      #  val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "            for val_batch in val_iter:\n",
    "                output = net(val_batch.premise,val_batch.hypothesis)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "                val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "                val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "                val_lengths += val_batch.batch_size\n",
    "            \n",
    "\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "        \n",
    "            val_loss_list.append(get_numpy(val_losses))\n",
    "            val_accs_list.append(get_numpy(val_accs))\n",
    "            val_iter_list.append(i)\n",
    "        \n",
    "            print(\"valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, get_numpy(val_losses), get_numpy(val_accs)))\n",
    "        \n",
    "            net.train()\n",
    "    \n",
    "        output = net(batch.premise,batch.hypothesis)\n",
    "        batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "        train_loss.append(get_numpy(batch_loss))\n",
    "        train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    " \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(),max_norm=0.5)\n",
    "        optimizer.step()\n",
    "    \n",
    "        if i % log_every == 0:        \n",
    "            print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "            train_loss_list.append(np.mean(train_loss))\n",
    "            train_accs_list.append(np.mean(train_accs))    \n",
    "            train_iter_list.append(i)\n",
    "            train_loss, train_accs = [], []\n",
    "    if max(val_accs_list[max_acc_idx:len(val_accs_list)]) > max_acc:\n",
    "        max_acc = max(val_accs_list[max_acc_idx:len(val_accs_list)])\n",
    "        max_acc_idx = np.argmax(max(val_accs_list[max_acc_idx:len(val_accs_list)]))\n",
    "    else:\n",
    "        print(\"Maximum validation accuracy: \" + str(max_acc))\n",
    "        reached_max = True\n",
    "        break\n",
    "    epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 494,
     "status": "error",
     "timestamp": 1546503847783,
     "user": {
      "displayName": "Uppahhh",
      "photoUrl": "",
      "userId": "09355873330880409745"
     },
     "user_tz": -60
    },
    "id": "Kswr_ZOxx4uY",
    "outputId": "09e13a86-0e8d-4345-db93-0437044f3096"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b269563106ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_iter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrain_iter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iter_list' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(int(len(train_iter_list)/(epochs)),len(train_iter_list)):\n",
    "    if train_iter_list[i] == 0:\n",
    "        train_iter_list[i] = train_iter_list[i-1]+1\n",
    "        continue\n",
    "    else:\n",
    "        train_iter_list[i] = train_iter_list[i-1] + log_every\n",
    "\n",
    "\n",
    "for i in range(int(len(val_iter_list)/epochs),len(val_iter_list)):\n",
    "    if val_iter_list[i] == 0:\n",
    "        val_iter_list[i] =val_iter_list[i-1]+1\n",
    "        continue\n",
    "    else:\n",
    "        val_iter_list[i] = val_iter_list[i-1] + eval_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o6-ruwlZJsUX"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_iter_list, train_loss_list, label='train_loss')\n",
    "plt.plot(val_iter_list, val_loss_list, label='valid_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_iter_list, train_accs_list, label='train_accs')\n",
    "plt.plot(val_iter_list, val_accs_list, label='valid_accs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "u7VHZe29kMgW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bag of words SSTrun.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
